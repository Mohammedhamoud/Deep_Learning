{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade arabic-reshaper\n",
    "import arabic_reshaper\n",
    "# !pip install python-bidi\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "from PIL import ImageFont, ImageDraw, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data 4')\n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['ineed', 'ambulance', 'where', 'street', 'work',\n",
    "                   'thankyou_bootcamp', 'in this', 'i_need_ambulance', 'i_want', 'report', 'accedint','I_want_to_report_accedient'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 10\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "# start_folder = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ineed': 0,\n",
       " 'ambulance': 1,\n",
       " 'where': 2,\n",
       " 'street': 3,\n",
       " 'work': 4,\n",
       " 'thankyou_bootcamp': 5,\n",
       " 'in this': 6,\n",
       " 'i_need_ambulance': 7,\n",
       " 'i_want': 8,\n",
       " 'report': 9,\n",
       " 'accedint': 10,\n",
       " 'final_sentetnce': 11}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 30, 1662)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360,)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 30, 1662)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 12)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 12)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train GRU Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(GRU(256, return_sequences=True, activation='tanh', input_shape=(30,1662)))\n",
    "model.add(GRU(128, return_sequences=False, activation='tanh'))\n",
    "# model.add(GRU(64, return_sequences=False, activation='tanh'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_6 (GRU)                 (None, 30, 256)           1474560   \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, 128)               148224    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 12)                396       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,633,516\n",
      "Trainable params: 1,633,516\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = './tmp/checkpoint'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "46/46 [==============================] - 6s 61ms/step - loss: 2.5320 - categorical_accuracy: 0.1087 - val_loss: 2.6453 - val_categorical_accuracy: 0.0517 - lr: 0.0010\n",
      "Epoch 2/2000\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 2.4887 - categorical_accuracy: 0.0870 - val_loss: 2.4491 - val_categorical_accuracy: 0.1552 - lr: 0.0010\n",
      "Epoch 3/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 2.3732 - categorical_accuracy: 0.1478 - val_loss: 2.2859 - val_categorical_accuracy: 0.1379 - lr: 0.0010\n",
      "Epoch 4/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 2.1898 - categorical_accuracy: 0.1217 - val_loss: 2.2059 - val_categorical_accuracy: 0.1552 - lr: 0.0010\n",
      "Epoch 5/2000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 2.1046 - categorical_accuracy: 0.2435 - val_loss: 2.8381 - val_categorical_accuracy: 0.0690 - lr: 0.0010\n",
      "Epoch 6/2000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 2.3891 - categorical_accuracy: 0.1174 - val_loss: 2.2425 - val_categorical_accuracy: 0.2241 - lr: 0.0010\n",
      "Epoch 7/2000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 2.0810 - categorical_accuracy: 0.1739 - val_loss: 2.1115 - val_categorical_accuracy: 0.1897 - lr: 0.0010\n",
      "Epoch 8/2000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 2.0527 - categorical_accuracy: 0.2609 - val_loss: 2.0323 - val_categorical_accuracy: 0.2241 - lr: 0.0010\n",
      "Epoch 9/2000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 1.8776 - categorical_accuracy: 0.3478 - val_loss: 2.3440 - val_categorical_accuracy: 0.1207 - lr: 0.0010\n",
      "Epoch 10/2000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 1.7543 - categorical_accuracy: 0.3783 - val_loss: 2.1357 - val_categorical_accuracy: 0.2069 - lr: 0.0010\n",
      "Epoch 11/2000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 1.5452 - categorical_accuracy: 0.4087 - val_loss: 1.9054 - val_categorical_accuracy: 0.2069 - lr: 0.0010\n",
      "Epoch 12/2000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 1.4169 - categorical_accuracy: 0.4870 - val_loss: 1.5022 - val_categorical_accuracy: 0.3966 - lr: 0.0010\n",
      "Epoch 13/2000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 1.2480 - categorical_accuracy: 0.5435 - val_loss: 1.2237 - val_categorical_accuracy: 0.5862 - lr: 0.0010\n",
      "Epoch 14/2000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 1.2464 - categorical_accuracy: 0.5304 - val_loss: 1.3617 - val_categorical_accuracy: 0.4138 - lr: 0.0010\n",
      "Epoch 15/2000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 1.2661 - categorical_accuracy: 0.5348 - val_loss: 1.2158 - val_categorical_accuracy: 0.5517 - lr: 0.0010\n",
      "Epoch 16/2000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.9845 - categorical_accuracy: 0.6435 - val_loss: 1.1289 - val_categorical_accuracy: 0.5345 - lr: 0.0010\n",
      "Epoch 17/2000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.8963 - categorical_accuracy: 0.6783 - val_loss: 0.9853 - val_categorical_accuracy: 0.7586 - lr: 0.0010\n",
      "Epoch 18/2000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.7814 - categorical_accuracy: 0.7435 - val_loss: 1.0910 - val_categorical_accuracy: 0.6724 - lr: 0.0010\n",
      "Epoch 19/2000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 0.7987 - categorical_accuracy: 0.7391 - val_loss: 0.9429 - val_categorical_accuracy: 0.6897 - lr: 0.0010\n",
      "Epoch 20/2000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.6960 - categorical_accuracy: 0.7870 - val_loss: 0.7751 - val_categorical_accuracy: 0.7931 - lr: 0.0010\n",
      "Epoch 21/2000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.5846 - categorical_accuracy: 0.8130 - val_loss: 0.8424 - val_categorical_accuracy: 0.7414 - lr: 0.0010\n",
      "Epoch 22/2000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.5845 - categorical_accuracy: 0.8130 - val_loss: 0.6432 - val_categorical_accuracy: 0.7931 - lr: 0.0010\n",
      "Epoch 23/2000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.6766 - categorical_accuracy: 0.7913 - val_loss: 0.8240 - val_categorical_accuracy: 0.7241 - lr: 0.0010\n",
      "Epoch 24/2000\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 0.5845 - categorical_accuracy: 0.8000 - val_loss: 0.9235 - val_categorical_accuracy: 0.6724 - lr: 0.0010\n",
      "Epoch 25/2000\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.6881 - categorical_accuracy: 0.7696\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "46/46 [==============================] - 1s 26ms/step - loss: 0.6881 - categorical_accuracy: 0.7696 - val_loss: 0.7893 - val_categorical_accuracy: 0.6379 - lr: 0.0010\n",
      "Epoch 26/2000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.4429 - categorical_accuracy: 0.8522 - val_loss: 0.6214 - val_categorical_accuracy: 0.7931 - lr: 5.0000e-04\n",
      "Epoch 27/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.3350 - categorical_accuracy: 0.9043 - val_loss: 0.7671 - val_categorical_accuracy: 0.6897 - lr: 5.0000e-04\n",
      "Epoch 28/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.2991 - categorical_accuracy: 0.9043 - val_loss: 0.5627 - val_categorical_accuracy: 0.8276 - lr: 5.0000e-04\n",
      "Epoch 29/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.2740 - categorical_accuracy: 0.9043 - val_loss: 0.5162 - val_categorical_accuracy: 0.8793 - lr: 5.0000e-04\n",
      "Epoch 30/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.2555 - categorical_accuracy: 0.9261 - val_loss: 0.5244 - val_categorical_accuracy: 0.8793 - lr: 5.0000e-04\n",
      "Epoch 31/2000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.2150 - categorical_accuracy: 0.9435 - val_loss: 0.5126 - val_categorical_accuracy: 0.8966 - lr: 5.0000e-04\n",
      "Epoch 32/2000\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 0.2499 - categorical_accuracy: 0.9217 - val_loss: 0.5635 - val_categorical_accuracy: 0.8966 - lr: 5.0000e-04\n",
      "Epoch 33/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.2006 - categorical_accuracy: 0.9435 - val_loss: 0.4697 - val_categorical_accuracy: 0.8966 - lr: 5.0000e-04\n",
      "Epoch 34/2000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.1758 - categorical_accuracy: 0.9652 - val_loss: 0.4328 - val_categorical_accuracy: 0.8793 - lr: 5.0000e-04\n",
      "Epoch 35/2000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.1799 - categorical_accuracy: 0.9435 - val_loss: 0.4259 - val_categorical_accuracy: 0.9138 - lr: 5.0000e-04\n",
      "Epoch 36/2000\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.1606 - categorical_accuracy: 0.9652 - val_loss: 0.4100 - val_categorical_accuracy: 0.9138 - lr: 5.0000e-04\n",
      "Epoch 37/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.1611 - categorical_accuracy: 0.9609 - val_loss: 0.8776 - val_categorical_accuracy: 0.7414 - lr: 5.0000e-04\n",
      "Epoch 38/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.1577 - categorical_accuracy: 0.9478 - val_loss: 0.3742 - val_categorical_accuracy: 0.9138 - lr: 5.0000e-04\n",
      "Epoch 39/2000\n",
      "46/46 [==============================] - 1s 27ms/step - loss: 0.1175 - categorical_accuracy: 0.9783 - val_loss: 0.3665 - val_categorical_accuracy: 0.9138 - lr: 5.0000e-04\n",
      "Epoch 40/2000\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.1129 - categorical_accuracy: 0.9696 - val_loss: 0.3303 - val_categorical_accuracy: 0.9483 - lr: 5.0000e-04\n",
      "Epoch 41/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0992 - categorical_accuracy: 0.9783 - val_loss: 0.4136 - val_categorical_accuracy: 0.8966 - lr: 5.0000e-04\n",
      "Epoch 42/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.1171 - categorical_accuracy: 0.9696 - val_loss: 0.3473 - val_categorical_accuracy: 0.9310 - lr: 5.0000e-04\n",
      "Epoch 43/2000\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1213 - categorical_accuracy: 0.9739\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.1213 - categorical_accuracy: 0.9739 - val_loss: 0.3766 - val_categorical_accuracy: 0.9138 - lr: 5.0000e-04\n",
      "Epoch 44/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0774 - categorical_accuracy: 0.9870 - val_loss: 0.3352 - val_categorical_accuracy: 0.9138 - lr: 2.5000e-04\n",
      "Epoch 45/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0701 - categorical_accuracy: 0.9870 - val_loss: 0.3220 - val_categorical_accuracy: 0.9483 - lr: 2.5000e-04\n",
      "Epoch 46/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0614 - categorical_accuracy: 0.9913 - val_loss: 0.3191 - val_categorical_accuracy: 0.9483 - lr: 2.5000e-04\n",
      "Epoch 47/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0586 - categorical_accuracy: 0.9913 - val_loss: 0.3390 - val_categorical_accuracy: 0.9483 - lr: 2.5000e-04\n",
      "Epoch 48/2000\n",
      "46/46 [==============================] - 1s 25ms/step - loss: 0.0547 - categorical_accuracy: 0.9913 - val_loss: 0.2916 - val_categorical_accuracy: 0.9483 - lr: 2.5000e-04\n",
      "Epoch 49/2000\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0590 - categorical_accuracy: 0.9913 - val_loss: 0.2985 - val_categorical_accuracy: 0.9483 - lr: 2.5000e-04\n",
      "Epoch 50/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0539 - categorical_accuracy: 0.9913 - val_loss: 0.2952 - val_categorical_accuracy: 0.9483 - lr: 2.5000e-04\n",
      "Epoch 51/2000\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0524 - categorical_accuracy: 0.9913\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "46/46 [==============================] - 1s 24ms/step - loss: 0.0524 - categorical_accuracy: 0.9913 - val_loss: 0.3111 - val_categorical_accuracy: 0.9483 - lr: 2.5000e-04\n",
      "Epoch 52/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0469 - categorical_accuracy: 0.9913 - val_loss: 0.3074 - val_categorical_accuracy: 0.9483 - lr: 1.2500e-04\n",
      "Epoch 53/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0459 - categorical_accuracy: 0.9913 - val_loss: 0.3051 - val_categorical_accuracy: 0.9483 - lr: 1.2500e-04\n",
      "Epoch 54/2000\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0450 - categorical_accuracy: 0.9913\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "46/46 [==============================] - 1s 22ms/step - loss: 0.0450 - categorical_accuracy: 0.9913 - val_loss: 0.3038 - val_categorical_accuracy: 0.9483 - lr: 1.2500e-04\n",
      "Epoch 55/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0434 - categorical_accuracy: 0.9913 - val_loss: 0.3048 - val_categorical_accuracy: 0.9483 - lr: 6.2500e-05\n",
      "Epoch 56/2000\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0431 - categorical_accuracy: 0.9913 - val_loss: 0.3068 - val_categorical_accuracy: 0.9483 - lr: 6.2500e-05\n",
      "Epoch 57/2000\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0427 - categorical_accuracy: 0.9913Restoring model weights from the end of the best epoch: 48.\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 0.0427 - categorical_accuracy: 0.9913 - val_loss: 0.3054 - val_categorical_accuracy: 0.9483 - lr: 6.2500e-05\n",
      "Epoch 00057: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f94d4ffa30>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=2000, batch_size=5, validation_split=.2, shuffle=True,\n",
    "          callbacks=[tb_callback,\n",
    "                     EarlyStopping(patience=9, verbose=1, restore_best_weights=True),\n",
    "                     ReduceLROnPlateau(factor=.5, patience=3, verbose=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'where'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accedint'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[63,  3],\n",
       "        [ 0,  6]],\n",
       "\n",
       "       [[64,  2],\n",
       "        [ 0,  6]],\n",
       "\n",
       "       [[66,  0],\n",
       "        [ 1,  5]],\n",
       "\n",
       "       [[66,  0],\n",
       "        [ 3,  3]],\n",
       "\n",
       "       [[66,  0],\n",
       "        [ 2,  4]],\n",
       "\n",
       "       [[66,  0],\n",
       "        [ 0,  6]],\n",
       "\n",
       "       [[66,  0],\n",
       "        [ 0,  6]],\n",
       "\n",
       "       [[65,  1],\n",
       "        [ 0,  6]],\n",
       "\n",
       "       [[66,  0],\n",
       "        [ 0,  6]],\n",
       "\n",
       "       [[66,  0],\n",
       "        [ 0,  6]],\n",
       "\n",
       "       [[66,  0],\n",
       "        [ 0,  6]],\n",
       "\n",
       "       [[66,  0],\n",
       "        [ 0,  6]]], dtype=int64)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4725808"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = CategoricalCrossentropy()\n",
    "cc(y_test, model.predict(X_test)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245),(16,117,245)]*4\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thankyou_bootcamp 100%\n"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "dict1 =  {\"احتاج سيارة اسعاف\": \"i_need_amubalance\" }\n",
    "\n",
    "cap = cv2.VideoCapture('./final 1.mp4')\n",
    "# cap = cv2.VideoCapture('./WhatsApp Video 2021-12-08 at 14.31.03.mp4')\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "out = cv2.VideoWriter('output2.avi', fourcc, 30.0, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),  \n",
    "                                                    int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=2) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: # end of video\n",
    "            break\n",
    "        \n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "#         print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        cv2.putText(image, str(len(sequence)), (30,100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            \n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "        #3. logic\n",
    "#             if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                print(actions[np.argmax(res)], format(res[np.argmax(res)], '.0%'))\n",
    "\n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            sequence = [] # start new sequence\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-3:]\n",
    "\n",
    "            # Viz probabilities\n",
    "#             image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "#         text = \"اود الابلاغ عن حادث\"\n",
    "# #         text = ' '.join([dict1.get(i, i) for i in sentence])\n",
    "        \n",
    "#         reshaped_text = arabic_reshaper.reshape(text)    # correct its shape\n",
    "#         bidi_text = get_display(reshaped_text)           # correct its direction\n",
    "#         fontpath = \"arial.ttf\" # <== https://www.freefontspro.com/14454/arial.ttf  \n",
    "#         font = ImageFont.truetype(fontpath, 32)\n",
    "#         img_pil = Image.fromarray(image)\n",
    "#         draw = ImageDraw.Draw(img_pil)\n",
    "#         draw.text((100, 80),bidi_text, font = font)\n",
    "#         img = np.array(img_pil)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        out.write(image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "\n",
    "    out.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imutils.video import FileVideoStream\n",
    "# from imutils.video import WebcamVideoStream\n",
    "# import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. New detection variables\n",
    "# sequence = []\n",
    "# sentence = []\n",
    "# predictions = []\n",
    "# threshold = 0.7\n",
    "# i = 0\n",
    "\n",
    "# # cap = cv2.VideoCapture('./ineed-9_hiz8v33F.mp4')\n",
    "# fvs = WebcamVideoStream(0).start()\n",
    "# # fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "# # out = cv2.VideoWriter('output2.avi', fourcc, 30.0, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),  \n",
    "# #                                                     int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "# # Set mediapipe model \n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=2) as holistic:\n",
    "#     while True:\n",
    "            \n",
    "#         # Read feed\n",
    "#         frame = fvs.read()\n",
    "        \n",
    "#         if i == 30:\n",
    "#             i = 1\n",
    "#         else:\n",
    "#             i+=1\n",
    "#         cv2.putText(image, str(i), (50,5), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "#         # Make detections\n",
    "#         image, results = mediapipe_detection(frame, holistic)\n",
    "# #         print(results)\n",
    "        \n",
    "#         # Draw landmarks\n",
    "#         draw_styled_landmarks(image, results)\n",
    "        \n",
    "#         # 2. Prediction logic\n",
    "#         keypoints = extract_keypoints(results)\n",
    "#         sequence.append(keypoints)\n",
    "#         sequence = sequence[-30:]\n",
    "        \n",
    "        \n",
    "#         if len(sequence) == 30:\n",
    "#             res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "# #             print(actions[np.argmax(res)])\n",
    "#             predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "#         #3. logic\n",
    "#             if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "#                 if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "#                     if len(sentence) > 0: \n",
    "#                         if actions[np.argmax(res)] != sentence[-1]:\n",
    "#                             sentence.append(actions[np.argmax(res)])\n",
    "#                     else:\n",
    "#                         sentence.append(actions[np.argmax(res)])\n",
    "                    \n",
    "#                     sequence = [] # start new sequence\n",
    "\n",
    "#             if len(sentence) > 5: \n",
    "#                 sentence = sentence[-5:]\n",
    "\n",
    "#             # Viz probabilities\n",
    "#             image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "#         cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "#         cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "#                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "# #         print(' '.join(sentence), end=' ')\n",
    "        \n",
    "#         # Show to screen\n",
    "#         cv2.imshow('OpenCV Feed', image)\n",
    "# #         out.write(image)\n",
    "        \n",
    "#         if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#             break\n",
    "    \n",
    "\n",
    "# #     out.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "#     fvs.stop()\n",
    "#     fvs.stream.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.destroyAllWindows()\n",
    "# fvs.stop()\n",
    "# fvs.stream.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
